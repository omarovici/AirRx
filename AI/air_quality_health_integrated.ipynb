{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Integrated Air Quality \u2192 Health Notebook\n",
        "\n",
        "This notebook is an end-to-end pipeline for the NASA Space Apps hackathon: it ingests TEMPO / OpenAQ pollutant CSVs, weather CSVs, and NLDAS netCDF (if available), creates region centroids, synthesizes health labels, trains pollutant forecast and health-impact models, and exports GeoJSON suitable for a frontend map.\n",
        "\n",
        "**Usage:** edit the configuration cell below to point to your files, then run cells from top to bottom. Cells include explanation and safety notes; where you lack real health data the notebook will generate synthetic labels so you can demo the full pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== CONFIG: set file paths and parameters =====\n",
        "ROOT = '/mnt/data'\n",
        "TEMPO_NO2_CSV = 'tempo_no2.csv'\n",
        "TEMPO_HCHO_CSV = 'tempo_hcho.csv'\n",
        "OPENAQ_CSV = 'openaq.csv'\n",
        "WEATHER_CSV = 'weatherdatamap.csv'\n",
        "NLDAS_FILE = 'NLDAS_FORA0125_H.A20250928.1200.020.nc'\n",
        "REGIONS_CSV = 'regions.csv'\n",
        "HEALTH_CSV = 'health_counts.csv'\n",
        "AGGREGATION = 'D'\n",
        "ROLL_WINDOWS = [1,3,6,24]\n",
        "LABEL_Z = 1.5\n",
        "TARGET_POLLUTANTS = ['no2','hcho']\n",
        "NUM_REGIONS_SAMPLE = 200\n",
        "print('CONFIG set. Edit paths above if needed.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import xarray as xr\n",
        "from datetime import datetime, timedelta\n",
        "warnings.filterwarnings('ignore')\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_recall_fscore_support, roc_auc_score\n",
        "import joblib\n",
        "print('Libraries imported')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load available data\n",
        "\n",
        "The notebook will attempt to load CSVs and the NLDAS NetCDF. If files are missing it will skip those steps but continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_read_csv(path, parse_time=True, time_col_candidates=['time','datetime','date']):\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        print(f'File not found: {path} - skipping')\n",
        "        return None\n",
        "    df = pd.read_csv(p)\n",
        "    tcol = None\n",
        "    for c in time_col_candidates:\n",
        "        if c in df.columns:\n",
        "            tcol = c; break\n",
        "    if parse_time and tcol is not None:\n",
        "        df[tcol] = pd.to_datetime(df[tcol])\n",
        "        df = df.rename(columns={tcol:'time'})\n",
        "    return df\n",
        "\n",
        "data = {}\n",
        "data['tempo_no2'] = safe_read_csv(TEMPO_NO2_CSV)\n",
        "data['tempo_hcho'] = safe_read_csv(TEMPO_HCHO_CSV)\n",
        "data['openaq'] = safe_read_csv(OPENAQ_CSV)\n",
        "data['weather'] = safe_read_csv(WEATHER_CSV)\n",
        "data['health'] = safe_read_csv(HEALTH_CSV)\n",
        "if Path(REGIONS_CSV).exists():\n",
        "    data['regions'] = pd.read_csv(REGIONS_CSV)\n",
        "    print('Loaded regions.csv with', len(data['regions']), 'regions')\n",
        "else:\n",
        "    data['regions'] = None\n",
        "    print('regions.csv not found; will attempt to generate from NLDAS if available')\n",
        "\n",
        "if Path(NLDAS_FILE).exists():\n",
        "    print('Opening NLDAS file:', NLDAS_FILE)\n",
        "    ds_nldas = xr.open_dataset(NLDAS_FILE)\n",
        "    print('NLDAS variables:', list(ds_nldas.data_vars))\n",
        "else:\n",
        "    ds_nldas = None\n",
        "    print('NLDAS file not found; NLDAS steps will be skipped')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Create regions (if missing)\n",
        "\n",
        "If `regions.csv` is not provided, create `regions_from_nldas.csv` by sampling grid cell centers from NLDAS or by restricting to a bounding box. This provides centroids for aggregation and demo mapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data.get('regions') is None:\n",
        "    if ds_nldas is None:\n",
        "        print('Cannot create regions: no regions.csv and no NLDAS file. Provide regions.csv or NLDAS.')\n",
        "    else:\n",
        "        print('Generating regions_from_nldas.csv from NLDAS grid...')\n",
        "        lats = ds_nldas['lat'].values\n",
        "        lons = ds_nldas['lon'].values\n",
        "        lon2d, lat2d = np.meshgrid(lons, lats)\n",
        "        lat_flat = lat2d.ravel(); lon_flat = lon2d.ravel()\n",
        "        N = min(NUM_REGIONS_SAMPLE, len(lat_flat))\n",
        "        idxs = np.linspace(0, len(lat_flat)-1, N, dtype=int)\n",
        "        regions_df = pd.DataFrame({'region_id':[f'R{i+1}' for i in range(N)], 'lat':lat_flat[idxs], 'lon':lon_flat[idxs]})\n",
        "        regions_df.to_csv('regions_from_nldas.csv', index=False)\n",
        "        data['regions'] = regions_df\n",
        "        print('Wrote regions_from_nldas.csv with', len(regions_df), 'regions')\n",
        "else:\n",
        "    print('Using provided regions.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Standardize missing values and basic cleaning\n",
        "\n",
        "Replace sentinel -9999 with NaN, inspect null percentages, and for station-like CSVs perform per-location interpolation. For NLDAS, perform temporal interpolation along time dimension (conservative)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k,v in list(data.items()):\n",
        "    if isinstance(v, pd.DataFrame):\n",
        "        data[k] = v.replace(-9999, np.nan)\n",
        "        print(k, 'loaded with shape', data[k].shape, 'nulls per col:\\n', data[k].isnull().sum().to_dict())\n",
        "\n",
        "if ds_nldas is not None:\n",
        "    try:\n",
        "        ds_nldas = ds_nldas.interpolate_na(dim='time', method='linear')\n",
        "        ds_nldas = ds_nldas.ffill(dim='time').bfill(dim='time')\n",
        "        print('NLDAS interpolation done')\n",
        "    except Exception as e:\n",
        "        print('NLDAS interpolation failed or too large:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Map pollutant/observation points to regions and aggregate\n",
        "\n",
        "We take TEMP/OPENAQ point-level data and map to nearest region centroid (region_id), then aggregate (mean) to daily/hourly depending on AGGREGATION."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nearest_region(df_points, regions_df, lat_col='lat', lon_col='lon'):\n",
        "    pts = df_points.copy()\n",
        "    # compute distances to all regions (brute force) and pick nearest\n",
        "    regs = regions_df[[lat_col, lon_col, 'region_id']].values\n",
        "    out_region = []\n",
        "    for i,row in pts.iterrows():\n",
        "        latp = row[lat_col]; lonp = row[lon_col]\n",
        "        d2 = (regions_df[lat_col]-latp)**2 + (regions_df[lon_col]-lonp)**2\n",
        "        out_region.append(regions_df.loc[d2.idxmin(), 'region_id'])\n",
        "    pts['region_id'] = out_region\n",
        "    return pts\n",
        "\n",
        "pts = []\n",
        "for src in ['tempo_no2','tempo_hcho','openaq']:\n",
        "    df = data.get(src)\n",
        "    if isinstance(df, pd.DataFrame) and 'time' in df.columns and 'lat' in df.columns and 'lon' in df.columns:\n",
        "        pts.append(df)\n",
        "\n",
        "if len(pts)==0 and ds_nldas is None:\n",
        "    print('No pollutant or NLDAS data available to proceed. Please provide at least one.')\n",
        "else:\n",
        "    if len(pts)>0:\n",
        "        all_pts = pd.concat(pts, ignore_index=True, sort=False)\n",
        "        print('Combined point data shape', all_pts.shape)\n",
        "        regions_df = data['regions']\n",
        "        pts_mapped = nearest_region(all_pts, regions_df)\n",
        "        pts_mapped['time'] = pd.to_datetime(pts_mapped['time'])\n",
        "        if AGGREGATION=='D':\n",
        "            pts_mapped['date'] = pts_mapped['time'].dt.floor('D')\n",
        "            agg = pts_mapped.groupby(['region_id','date']).mean().reset_index()\n",
        "        else:\n",
        "            pts_mapped['hour'] = pts_mapped['time'].dt.floor('H')\n",
        "            agg = pts_mapped.groupby(['region_id','hour']).mean().reset_index()\n",
        "        data['pollutant_regions'] = agg\n",
        "        print('Aggregated pollutant data to regions:', agg.shape)\n",
        "    if ds_nldas is not None:\n",
        "        regions_df = data['regions']\n",
        "        recs = []\n",
        "        times = pd.to_datetime(ds_nldas['time'].values)\n",
        "        for rid, r in regions_df.iterrows():\n",
        "            lat_idx = int(np.abs(ds_nldas['lat'].values - r['lat']).argmin())\n",
        "            lon_idx = int(np.abs(ds_nldas['lon'].values - r['lon']).argmin())\n",
        "            tmp = pd.DataFrame({'time': times})\n",
        "            for v in ['Tair','Qair','PSurf','Wind_E','Wind_N','Rainf','CRainf_frac','SWdown','PotEvap','CAPE']:\n",
        "                if v in ds_nldas:\n",
        "                    tmp[v] = ds_nldas[v][:, lat_idx, lon_idx].values\n",
        "            tmp['region_id'] = r['region_id']; tmp['lat']=r['lat']; tmp['lon']=r['lon']\n",
        "            recs.append(tmp)\n",
        "        reg_all = pd.concat(recs, ignore_index=True)\n",
        "        if AGGREGATION=='D':\n",
        "            reg_all['date'] = pd.to_datetime(reg_all['time']).dt.floor('D')\n",
        "            reg_daily = reg_all.groupby(['region_id','date']).mean().reset_index()\n",
        "            data['nldas_regions'] = reg_daily\n",
        "            print('Extracted NLDAS -> regions daily shape', reg_daily.shape)\n",
        "        else:\n",
        "            data['nldas_regions'] = reg_all\n",
        "            print('Extracted NLDAS -> regions hourly shape', reg_all.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Clean, fill small gaps, and engineer features\n",
        "\n",
        "Compute wind speed/direction, rolling rainfall windows, convective-weighted intensity, heat index if possible, and lag features for pollutants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_features_regions(df):\n",
        "    df = df.copy()\n",
        "    if 'Wind_E' in df.columns and 'Wind_N' in df.columns:\n",
        "        df['wind_speed'] = np.sqrt(df['Wind_E']**2 + df['Wind_N']**2)\n",
        "        df['wind_dir_to'] = (np.degrees(np.arctan2(df['Wind_E'], df['Wind_N'])) % 360)\n",
        "    if 'Rainf' in df.columns:\n",
        "        df = df.sort_values('date' if 'date' in df.columns else 'time')\n",
        "        df['rain_roll_3'] = df.groupby('region_id')['Rainf'].rolling(window=3, min_periods=1).sum().reset_index(0,drop=True)\n",
        "        if 'CRainf_frac' in df.columns:\n",
        "            df['conv_intensity_3'] = df['rain_roll_3'] * df['CRainf_frac']\n",
        "    if all(c in df.columns for c in ['Tair','Qair','PSurf']):\n",
        "        T_K = df['Tair']; T_C = T_K - 273.15\n",
        "        es = 6.112 * np.exp((17.67 * T_C) / (T_C + 243.5))\n",
        "        p_hpa = df['PSurf'] / 100.0\n",
        "        e = df['Qair'] * p_hpa / (0.622 + 0.378*df['Qair'] + 1e-12)\n",
        "        df['RH'] = np.clip((e / es) * 100.0, 0, 100)\n",
        "        T_F = (T_C * 9/5) + 32\n",
        "        RH = df['RH']\n",
        "        HI = -42.379 + 2.04901523*T_F + 10.14333127*RH - 0.22475541*T_F*RH - 6.83783e-3*T_F**2 - 5.481717e-2*RH**2 + 1.22874e-3*T_F**2*RH + 8.5282e-4*T_F*RH**2 - 1.99e-6*T_F**2*RH**2\n",
        "        df['heat_index_C'] = (HI - 32) * 5/9\n",
        "    return df\n",
        "\n",
        "regions_table = None\n",
        "if data.get('nldas_regions') is not None:\n",
        "    regions_table = data['nldas_regions']\n",
        "    if 'date' not in regions_table.columns and 'time' in regions_table.columns:\n",
        "        regions_table['date'] = pd.to_datetime(regions_table['time']).dt.floor('D')\n",
        "    regions_table = add_features_regions(regions_table)\n",
        "    data['regions_features'] = regions_table\n",
        "elif data.get('pollutant_regions') is not None:\n",
        "    regions_table = data['pollutant_regions']\n",
        "    if 'date' not in regions_table.columns and 'time' in regions_table.columns:\n",
        "        regions_table['date'] = pd.to_datetime(regions_table['time']).dt.floor('D')\n",
        "    regions_table = add_features_regions(regions_table)\n",
        "    data['regions_features'] = regions_table\n",
        "else:\n",
        "    print('No region-level table available to engineer features')\n",
        "    data['regions_features'] = None\n",
        "\n",
        "if data['regions_features'] is not None:\n",
        "    print('Regions features shape:', data['regions_features'].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Create synthetic health labels (two strategies)\n",
        "\n",
        "Strategy A: pollutant z-score per region. Strategy B: compound event (pollutant > percentile AND low wind). Notebook produces both and combines them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def synth_labels_zscore(df, pollutant='no2', z_thresh=1.5):\n",
        "    df = df.copy()\n",
        "    base = df.groupby('region_id')[pollutant].agg(['mean','std']).reset_index().rename(columns={'mean':'bmean','std':'bstd'})\n",
        "    df = df.merge(base, on='region_id', how='left')\n",
        "    df['z'] = (df[pollutant] - df['bmean']) / (df['bstd'].replace(0,np.nan)+1e-9)\n",
        "    df['label_z'] = (df['z'] > z_thresh).astype(int)\n",
        "    return df\n",
        "\n",
        "def synth_labels_compound(df, pollutant='no2', pct=0.9, wind_thresh=2.0):\n",
        "    df = df.copy()\n",
        "    pr = df.groupby('region_id')[pollutant].transform(lambda x: x.quantile(pct))\n",
        "    df['label_compound'] = (((df[pollutant] >= pr) & (df.get('wind_speed', 0) < wind_thresh)).astype(int))\n",
        "    return df\n",
        "\n",
        "if data.get('regions_features') is not None:\n",
        "    rf = data['regions_features'].copy()\n",
        "    pollutant_col = 'no2' if 'no2' in rf.columns else ('hcho' if 'hcho' in rf.columns else None)\n",
        "    if pollutant_col is None:\n",
        "        print('No pollutant column found in regions_features; creating synthetic pollutant using random noise for demo')\n",
        "        np.random.seed(0)\n",
        "        rf['no2'] = np.random.gamma(shape=2.0, scale=10.0, size=len(rf))\n",
        "        pollutant_col = 'no2'\n",
        "    rf = synth_labels_zscore(rf, pollutant=pollutant_col, z_thresh=LABEL_Z)\n",
        "    rf = synth_labels_compound(rf, pollutant=pollutant_col)\n",
        "    rf['label'] = ((rf['label_z'] + rf['label_compound']) > 0).astype(int)\n",
        "    data['regions_labeled'] = rf\n",
        "    print('Synthetic labels created; sample:')\n",
        "    cols_show = [c for c in ['region_id','date', pollutant_col, 'z','label_z','label_compound','label'] if c in rf.columns]\n",
        "    print(rf[cols_show].head())\n",
        "else:\n",
        "    print('No regions_features available, cannot create synthetic labels')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Prepare model dataset: lag features and train/test split\n",
        "\n",
        "We create lagged pollutant features (t-1, t-2, etc.) and then split time-aware for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_model_df(rf, pollutant='no2', lags=[1,2,3]):\n",
        "    df = rf.copy().sort_values(['region_id','date'])\n",
        "    for lag in lags:\n",
        "        df[f'{pollutant}_lag{lag}'] = df.groupby('region_id')[pollutant].shift(lag)\n",
        "    df['label_next'] = df.groupby('region_id')['label'].shift(-1)\n",
        "    df = df.dropna(subset=[f'{pollutant}_lag1', 'label_next'])\n",
        "    return df\n",
        "\n",
        "model_df = None\n",
        "if data.get('regions_labeled') is not None:\n",
        "    model_df = prepare_model_df(data['regions_labeled'], 'no2', lags=[1,2,3])\n",
        "    print('Model DF shape:', model_df.shape)\n",
        "else:\n",
        "    print('No labeled regions to prepare model DF')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Train models\n",
        "\n",
        "Two models: pollutant forecast (regression) and health-impact classifier (binary). We use LightGBM for both for speed and explainability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {}\n",
        "if model_df is not None and len(model_df)>0:\n",
        "    feat_pollutant = [c for c in model_df.columns if 'lag' in c or c in ['Tair','wind_speed','rain_roll_3']]\n",
        "    target_poll = 'no2'\n",
        "    split = int(0.8 * len(model_df))\n",
        "    X = model_df[feat_pollutant].fillna(0)\n",
        "    y = model_df[target_poll]\n",
        "    dtrain = lgb.Dataset(X.iloc[:split], label=y.iloc[:split])\n",
        "    params_reg = {'objective':'regression','metric':'rmse','verbosity':-1}\n",
        "    m_reg = lgb.train(params_reg, dtrain, num_boost_round=200)\n",
        "    ypred = m_reg.predict(X.iloc[split:])\n",
        "    print('Pollutant reg RMSE:', np.sqrt(mean_squared_error(y.iloc[split:], ypred)))\n",
        "    models['pollutant_reg'] = m_reg\n",
        "    joblib.dump(m_reg, 'pollutant_reg.joblib')\n",
        "\n",
        "    feat_health = feat_pollutant.copy()\n",
        "    Xh = model_df[feat_health].fillna(0)\n",
        "    yh = model_df['label_next'].astype(int)\n",
        "    dtrain_h = lgb.Dataset(Xh.iloc[:split], label=yh.iloc[:split])\n",
        "    params_clf = {'objective':'binary','metric':'auc','verbosity':-1}\n",
        "    m_clf = lgb.train(params_clf, dtrain_h, num_boost_round=200)\n",
        "    yprob = m_clf.predict(Xh.iloc[split:])\n",
        "    yhat = (yprob>=0.5).astype(int)\n",
        "    p,r,f,_ = precision_recall_fscore_support(yh.iloc[split:], yhat, average='binary', zero_division=0)\n",
        "    auc = roc_auc_score(yh.iloc[split:], yprob)\n",
        "    print('Health classifier P,R,F1,AUC:', p, r, f, auc)\n",
        "    models['health_clf'] = m_clf\n",
        "    joblib.dump(m_clf, 'health_clf.joblib')\n",
        "else:\n",
        "    print('No training data available; ensure regions and pollutant columns exist')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Produce GeoJSON predictions for the latest date\n",
        "\n",
        "Use trained health classifier to predict probability for latest date per region and export GeoJSON with advice categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preds_to_geojson(model_clf, model_df, regions_df, features, out_path='predictions_last_day.geojson'):\n",
        "    latest = model_df.groupby('region_id').apply(lambda g: g.sort_values('date').iloc[-1]).reset_index(drop=True)\n",
        "    Xpred = latest[features].fillna(0)\n",
        "    probs = model_clf.predict(Xpred)\n",
        "    latest['pred_prob'] = probs\n",
        "    feats = []\n",
        "    for _, r in latest.iterrows():\n",
        "        reg = regions_df[regions_df['region_id']==r['region_id']].iloc[0]\n",
        "        lat = float(reg['lat']); lon = float(reg['lon'])\n",
        "        p = float(r['pred_prob'])\n",
        "        cat = 'Low' if p<0.2 else ('Moderate' if p<0.5 else 'High')\n",
        "        advice = 'No action' if cat=='Low' else ('Sensitive groups reduce outdoor activity' if cat=='Moderate' else 'Avoid outdoor activity; clinics on alert')\n",
        "        feats.append({'type':'Feature','geometry':{'type':'Point','coordinates':[lon, lat]},'properties':{'region_id':r['region_id'],'pred_prob':p,'category':cat,'advice':advice}})\n",
        "    geo = {'type':'FeatureCollection','features':feats}\n",
        "    with open(out_path,'w') as f:\n",
        "        json.dump(geo, f)\n",
        "    print('Wrote', out_path)\n",
        "    return out_path\n",
        "\n",
        "if 'health_clf' in models and model_df is not None:\n",
        "    features_used = feat_health\n",
        "    out = preds_to_geojson(models['health_clf'], model_df, data['regions'], features_used)\n",
        "else:\n",
        "    print('No health classifier available to produce predictions')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Done\n",
        "\n",
        "Files produced (if steps ran):\n",
        "- regions_from_nldas.csv (if created)\n",
        "- regions_features table (in memory)\n",
        "- regions_labeled (in memory)\n",
        "- pollutant_reg.joblib, health_clf.joblib (saved models if training ran)\n",
        "- predictions_last_day.geojson (if model trained)\n",
        "\n",
        "Next: build FastAPI to serve predictions or React/Leaflet frontend to visualize geojson. Remember to clearly state synthetic labels if using them for demo."
      ]
    }
  ]
}